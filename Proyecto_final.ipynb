{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZdAI0uDtVXL",
        "outputId": "06a56f0d-eddc-47c3-92f1-5fdeeb32e2e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai==0.28 in /usr/local/lib/python3.10/dist-packages (0.28.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.66.5)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.10.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2024.7.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install openai==0.28\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "print(openai.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8C3Iahwd44FY",
        "outputId": "141894cf-ba80-4d09-b90a-c3860d472e5c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.28.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "openai.api_key = \"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\"\n"
      ],
      "metadata": {
        "id": "_zSTdUxIt-rj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Función para generar un prompt de texto estático usando la nueva API\n",
        "def generar_prompt(prompt, max_tokens, temperature):\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"Eres un asistente útil.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        max_tokens=max_tokens,\n",
        "        temperature=temperature\n",
        "    )\n",
        "    return response.choices[0].message['content'].strip()"
      ],
      "metadata": {
        "id": "F6IdKvIRykCV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Función para generar un prompt dinámico basado en el comportamiento del usuario\n",
        "def generar_prompt_dinamico(comportamiento_usuario):\n",
        "    if comportamiento_usuario == \"navegando_ropa\":\n",
        "        prompt = \"Estás navegando en nuestra sección de ropa. ¡Aprovecha nuestras ofertas en camisas y vestidos con hasta un 50% de descuento!\"\n",
        "    elif comportamiento_usuario == \"navegando_electronica\":\n",
        "        prompt = \"¿Estás buscando un accesorio para tu dispositivo? Tenemos ofertas en cargadores, fundas y más.\"\n",
        "    elif comportamiento_usuario == \"navegación_larga\":\n",
        "        prompt = \"Llevas un tiempo explorando. ¡No te pierdas nuestras ofertas por tiempo limitado!\"\n",
        "    else:\n",
        "        prompt = \"¿En qué categoría te gustaría explorar más? Descubre nuestras mejores ofertas.\"\n",
        "    return prompt"
      ],
      "metadata": {
        "id": "CvAhwK7fzqYk"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulación de comportamiento del usuario\n",
        "comportamiento_usuario = \"navegando_ropa\"  # Puedes cambiar esto según el comportamiento del usuario\n",
        "\n",
        "# Generar prompt basado en el comportamiento\n",
        "prompt_dinamico = generar_prompt_dinamico(comportamiento_usuario)"
      ],
      "metadata": {
        "id": "LDrCPsebzs-Y"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuraciones para la generación de prompts\n",
        "configuraciones = [\n",
        "    {\"max_tokens\": 50, \"temperature\": 0.5},\n",
        "    {\"max_tokens\": 100, \"temperature\": 0.7},\n",
        "    {\"max_tokens\": 150, \"temperature\": 1.0},\n",
        "]"
      ],
      "metadata": {
        "id": "pWPuoRsEzwaN"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Probar cada configuración con el prompt dinámico\n",
        "resultados = []\n",
        "for config in configuraciones:\n",
        "    respuesta = generar_prompt(prompt_dinamico, max_tokens=config[\"max_tokens\"], temperature=config[\"temperature\"])\n",
        "    resultados.append((config, respuesta))"
      ],
      "metadata": {
        "id": "2pIADIoLz3cQ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostrar los resultados\n",
        "for config, respuesta in resultados:\n",
        "    print(f\"Config: {config}\")\n",
        "    print(f\"Respuesta: {respuesta}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSNXAusG5asT",
        "outputId": "4480e101-60a2-4c0e-853d-5cb69160d211"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Config: {'max_tokens': 50, 'temperature': 0.5}\n",
            "Respuesta: ¡Gracias por la información! ¿Puedo ayudarte a encontrar algo en particular dentro de la sección de ropa?\n",
            "\n",
            "Config: {'max_tokens': 100, 'temperature': 0.7}\n",
            "Respuesta: ¡Gracias por la información! ¿Hay algún estilo en particular que esté en oferta y que recomiendes?\n",
            "\n",
            "Config: {'max_tokens': 150, 'temperature': 1.0}\n",
            "Respuesta: ¡Hola! ¡Eso suena genial! Si necesitas ayuda para encontrar algo específico o tienes alguna duda sobre tallas o estilos, estaré encantado de ayudarte. ¿En qué más te puedo asistir hoy en tu búsqueda de ropa?\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"\"\"\n",
        "\n",
        "Justificación basada en datos\n",
        "Según estudios recientes, la personalización en tiempo real puede aumentar la tasa de conversión en un 20-30%. Al adaptar los prompts según el comportamiento del usuario, se espera que el proyecto mejore significativamente la interacción y satisfacción del usuario, lo que a su vez podría resultar en un aumento en las ventas.\n",
        "\n",
        "Fase de pruebas y ajustes\n",
        "Durante la fase de pruebas, se llevará a cabo un A/B testing donde un grupo de usuarios recibirá prompts personalizados en tiempo real, mientras que otro grupo recibirá prompts estáticos. Se recopilarán métricas de conversión, tiempo de navegación y satisfacción del usuario para ajustar y optimizar los prompts. \"\"\""
      ],
      "metadata": {
        "id": "cWkGu6Ju8n_G"
      }
    }
  ]
}